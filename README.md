# naive_bayes
Readme没想到拖了这么久，为了答谢目前唯一的star，决定详细补一下
右键分类是朴素贝叶斯的一个典型应用。朴素(naive)贝叶斯之所以naive，是因为它天真地以为各个因素之间是独立的。如邮件分类中，单词A和单词B的出现是独立的，但是实际显然不是如此，贝叶斯网络成功克服了这个先天缺陷。
1.在垃圾邮件分类中，我们事先在训练集中统计一些词汇在垃圾邮件和正常邮件中出现的概率，在这一步可以同一将大写字母转换为小写(虽然这样也会带入误差，如China和china是两个截然不同的意思，但是毕竟是少数)，去除一些没有意义的词汇，如a、the等。
2.朴素贝叶斯的关键是下面这个公式：p(w|x) = p(x|w)p(w) / p(x)。先来推导一下这个公式，p(w|x)的意思是在x事件发生的情况下w发生的概率，那么p(w|x)p(x) = p(w,x) = p(x|w)p(w),p(w,x)是x事件和w事件的联合分布。也许有些摸不着头脑，结合邮件分类来看一下。w0可以是邮件是垃圾邮件这个事件，x是某个词汇（如：sell）出现这个事件。那么公式就可以解释为邮件中包含了sell这个单词的情况下这封邮件是垃圾邮件的概率就等于垃圾邮件中sell出现的概率乘上垃圾邮件的概率再除以sell单词出现的总概率。举一个具体的数值。
  如从训练集中得到sell在垃圾邮件中出现的概率是0.6，即60%的垃圾邮件中都包含sell这个单词，而sell在正常邮件中出现的概率是0.1。
  垃圾邮件的概率是0.2，即训练集中20%的邮件是垃圾邮件，正常邮件概率就是0.8
  sell单词在邮件中出现的总概率就是0.2*0.6+0.1*0.8 = 0.2,即全部邮件中20%的邮件是含有sell这个单词的。
  那么如果我们发现一封邮件中出现了sell这个单词，它是垃圾邮件的概率即在sell词汇出现的情况下邮件是垃圾邮件的概率p(垃圾邮件|sell单词) = p(sell单词|垃圾邮件)p(垃圾邮件) / p(sell单词) = 0.6 × 0.2 / 0.2 = 0.6
  同理我们可以计算该邮件不是垃圾邮件的概率，p(非垃圾邮件|sell单词) = p(sell单词|非垃圾邮件)p（非垃圾邮件）/p（sell单词）= 0.1 * 0.8 / 0.2 = 0.4。因为p(垃圾邮件|sell单词) = 0.6 > 0.4 = p(非垃圾邮件|sell单词),所以我们认为这封邮件是垃圾邮件。
  
  但是发现不科学的啊，这样只要邮件中有sell单词就一定是垃圾邮件了？邮件中可不是只有这一个单词，我们像统计sell单词一样统计所有的词汇，就要用的前面naive的假设了，即单词之间的出现是独立的，那么公式就可以写作：
              p(w|(x1,x2...xn)) = p((x1,x2...xn)|w) (w) / p(x1,x2...xn)
p(x1,x2...xn)就是单词x1,x2..xn一起出现的概率，因为前面的独立假设，p(x1,x2...xn)就等于p(x1)p(x2)...p(xn);公式就可以写作：
              p(w|x1,x2...xn) = p(x1|w)p(x2|w)...p(xn|w)p(w) / p(x1)p(x2)...p(xn)
 其余都和前面sell一个单词操作相同。偷懒不写了，其实上班摸鱼时间到了。
 ——————————————————————————————————————————————————————————————————————————————————————————————————————
 可以实现自学习，如当我们预测一封邮件后，把其中词汇出现的情况加入到训练模型中，如预测一封邮件是垃圾邮件，那么sell在垃圾邮件中出现的概率就不再是0.4了;
 
 如果发现测试集中的某些词汇训练集中没有怎么办？分母不能为0的呀，采用啦普拉死平滑，该词汇出现次数记为1,总的词汇量也要加一
 
 可以统计词汇在邮件中的出现次数而不仅仅是不是出现，然后依据次数分配不同的权重
 
 ———————————————————————————————————————————————————————————————————————————————————————————————————————
 还不太熟悉readme的格式和内容，见谅
